{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "from sklearn import neural_network\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sepia(file_path): \n",
    "#     img = Image.open(file_path)\n",
    "#     width, height = img.size\n",
    "\n",
    "#     pixels = img.load()\n",
    "\n",
    "#     for py in range(height):\n",
    "#         for px in range(width):\n",
    "#             r, g, b = img.getpixel((px, py))\n",
    "\n",
    "#             tr = int(0.393 * r + 0.769 * g + 0.189 * b)\n",
    "#             tg = int(0.349 * r + 0.686 * g + 0.168 * b)\n",
    "#             tb = int(0.272 * r + 0.534 * g + 0.131 * b)\n",
    "\n",
    "#             if tr > 255:\n",
    "#                 tr = 255\n",
    "\n",
    "#             if tg > 255:\n",
    "#                 tg = 255\n",
    "\n",
    "#             if tb > 255:\n",
    "#                 tb = 255\n",
    "\n",
    "#             pixels[px, py] = (tr,tg,tb)\n",
    "\n",
    "#     return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = \"./data\"\n",
    "# sepia_folder = \"./sepia\"\n",
    "# for filename in os.listdir(data_folder):\n",
    "#     file_path = os.path.join(data_folder, filename)\n",
    "#     print(file_path)\n",
    "#     img = sepia(file_path)\n",
    "#     sepia_file_path = os.path.join(sepia_folder, filename)\n",
    "#     print(sepia_file_path)\n",
    "#     img.save(sepia_file_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - normal\n",
    "1 - sepia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/0708.png' './sepia/0002.png' './data/0674.png' ...\n",
      " './data/0673.png' './data/0328.png' './sepia/0615.png']\n",
      "[0 1 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for filename in os.listdir(\"./data\"):\n",
    "    file_path = os.path.join(\"./data\", filename)\n",
    "    inputs.append(file_path)\n",
    "    outputs.append(0)\n",
    "\n",
    "for filename in os.listdir(\"./sepia\"):\n",
    "    file_path = os.path.join(\"./sepia\", filename)\n",
    "    inputs.append(file_path)\n",
    "    outputs.append(1)\n",
    "\n",
    "noData = len(inputs)\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "outputs = np.array(outputs)\n",
    "\n",
    "permutation = np.random.permutation(noData)\n",
    "\n",
    "inputs = inputs[permutation]\n",
    "outputs = outputs[permutation]\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data(inputs, outputs):\n",
    "    indexes = [i for i in range(len(inputs))]\n",
    "\n",
    "    train_samples = np.random.choice(indexes, int(0.8 * len(inputs)))\n",
    "    test_samples = [i for i in indexes if not i in train_samples]\n",
    "    \n",
    "    random.shuffle(test_samples)\n",
    "\n",
    "    train_inputs = [inputs[i] for i in train_samples]\n",
    "    train_outputs = [outputs[i] for i in train_samples]\n",
    "\n",
    "    test_inputs = [inputs[i] for i in test_samples]\n",
    "    test_outputs = [outputs[i] for i in test_samples]\n",
    "\n",
    "    return train_inputs, train_outputs, test_inputs, test_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_outputs, test_inputs, test_outputs = split_data(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(images):\n",
    "    parameters = []\n",
    "    for image in images:\n",
    "        image = Image.open(image)\n",
    "        # display(image)\n",
    "        image_parameters = []\n",
    "        for pixel in list(image.getdata()):\n",
    "            r, g, b = pixel[:3]  # Extract RGB values\n",
    "            image_parameters.extend([r, g, b])\n",
    "        parameters.append(image_parameters)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68618179\n",
      "Iteration 2, loss = 0.67546224\n",
      "Iteration 3, loss = 0.67266669\n",
      "Iteration 4, loss = 0.66967541\n",
      "Iteration 5, loss = 0.66576770\n",
      "Iteration 6, loss = 0.66091063\n",
      "Iteration 7, loss = 0.65565373\n",
      "Iteration 8, loss = 0.65162748\n",
      "Iteration 9, loss = 0.64788102\n",
      "Iteration 10, loss = 0.64393624\n",
      "Iteration 11, loss = 0.63953426\n",
      "Iteration 12, loss = 0.63406266\n",
      "Iteration 13, loss = 0.62898574\n",
      "Iteration 14, loss = 0.62360221\n",
      "Iteration 15, loss = 0.61878234\n",
      "Iteration 16, loss = 0.61229093\n",
      "Iteration 17, loss = 0.60761521\n",
      "Iteration 18, loss = 0.60171637\n",
      "Iteration 19, loss = 0.59646495\n",
      "Iteration 20, loss = 0.59095976\n",
      "Iteration 21, loss = 0.58572492\n",
      "Iteration 22, loss = 0.58081884\n",
      "Iteration 23, loss = 0.57700635\n",
      "Iteration 24, loss = 0.57141090\n",
      "Iteration 25, loss = 0.56624872\n",
      "Iteration 26, loss = 0.56122914\n",
      "Iteration 27, loss = 0.55598840\n",
      "Iteration 28, loss = 0.55133328\n",
      "Iteration 29, loss = 0.54675444\n",
      "Iteration 30, loss = 0.54190903\n",
      "Iteration 31, loss = 0.53756261\n",
      "Iteration 32, loss = 0.53275467\n",
      "Iteration 33, loss = 0.52797919\n",
      "Iteration 34, loss = 0.52332163\n",
      "Iteration 35, loss = 0.51954125\n",
      "Iteration 36, loss = 0.51617834\n",
      "Iteration 37, loss = 0.51029098\n",
      "Iteration 38, loss = 0.50653136\n",
      "Iteration 39, loss = 0.50277938\n",
      "Iteration 40, loss = 0.49827792\n",
      "Iteration 41, loss = 0.49508181\n",
      "Iteration 42, loss = 0.49094719\n",
      "Iteration 43, loss = 0.48824707\n",
      "Iteration 44, loss = 0.48290203\n",
      "Iteration 45, loss = 0.47915618\n",
      "Iteration 46, loss = 0.47578502\n",
      "Iteration 47, loss = 0.47232356\n",
      "Iteration 48, loss = 0.46913534\n",
      "Iteration 49, loss = 0.46598052\n",
      "Iteration 50, loss = 0.46285489\n",
      "Iteration 51, loss = 0.45874097\n",
      "Iteration 52, loss = 0.45573617\n",
      "Iteration 53, loss = 0.45287535\n",
      "Iteration 54, loss = 0.44988821\n",
      "Iteration 55, loss = 0.44691699\n",
      "Iteration 56, loss = 0.44397266\n",
      "Iteration 57, loss = 0.44154299\n",
      "Iteration 58, loss = 0.43886676\n",
      "Iteration 59, loss = 0.43539919\n",
      "Iteration 60, loss = 0.43196275\n",
      "Iteration 61, loss = 0.42863711\n",
      "Iteration 62, loss = 0.42536667\n",
      "Iteration 63, loss = 0.42237677\n",
      "Iteration 64, loss = 0.42018166\n",
      "Iteration 65, loss = 0.41629005\n",
      "Iteration 66, loss = 0.41427091\n",
      "Iteration 67, loss = 0.41175820\n",
      "Iteration 68, loss = 0.40842606\n",
      "Iteration 69, loss = 0.40571982\n",
      "Iteration 70, loss = 0.40303354\n",
      "Iteration 71, loss = 0.40004296\n",
      "Iteration 72, loss = 0.39855441\n",
      "Iteration 73, loss = 0.39560833\n",
      "Iteration 74, loss = 0.39290319\n",
      "Iteration 75, loss = 0.39080429\n",
      "Iteration 76, loss = 0.38750537\n",
      "Iteration 77, loss = 0.38654301\n",
      "Iteration 78, loss = 0.38204272\n",
      "Iteration 79, loss = 0.38019132\n",
      "Iteration 80, loss = 0.37763438\n",
      "Iteration 81, loss = 0.37485202\n",
      "Iteration 82, loss = 0.37378400\n",
      "Iteration 83, loss = 0.36967941\n",
      "Iteration 84, loss = 0.36780985\n",
      "Iteration 85, loss = 0.36485173\n",
      "Iteration 86, loss = 0.36164412\n",
      "Iteration 87, loss = 0.35915286\n",
      "Iteration 88, loss = 0.35756423\n",
      "Iteration 89, loss = 0.35528898\n",
      "Iteration 90, loss = 0.35185125\n",
      "Iteration 91, loss = 0.35041888\n",
      "Iteration 92, loss = 0.34721716\n",
      "Iteration 93, loss = 0.34546781\n",
      "Iteration 94, loss = 0.34227720\n",
      "Iteration 95, loss = 0.34093227\n",
      "Iteration 96, loss = 0.33787000\n",
      "Iteration 97, loss = 0.33459339\n",
      "Iteration 98, loss = 0.33229422\n",
      "Iteration 99, loss = 0.32935549\n",
      "Iteration 100, loss = 0.32704882\n",
      "Iteration 101, loss = 0.32494550\n",
      "Iteration 102, loss = 0.32271620\n",
      "Iteration 103, loss = 0.31977586\n",
      "Iteration 104, loss = 0.31767315\n",
      "Iteration 105, loss = 0.31610153\n",
      "Iteration 106, loss = 0.31286333\n",
      "Iteration 107, loss = 0.31076817\n",
      "Iteration 108, loss = 0.30774211\n",
      "Iteration 109, loss = 0.30594057\n",
      "Iteration 110, loss = 0.30437621\n",
      "Iteration 111, loss = 0.30125941\n",
      "Iteration 112, loss = 0.29856412\n",
      "Iteration 113, loss = 0.29594351\n",
      "Iteration 114, loss = 0.29399733\n",
      "Iteration 115, loss = 0.29115671\n",
      "Iteration 116, loss = 0.28974744\n",
      "Iteration 117, loss = 0.28731989\n",
      "Iteration 118, loss = 0.28398561\n",
      "Iteration 119, loss = 0.28283209\n",
      "Iteration 120, loss = 0.28021163\n",
      "Iteration 121, loss = 0.27809304\n",
      "Iteration 122, loss = 0.27448197\n",
      "Iteration 123, loss = 0.27358419\n",
      "Iteration 124, loss = 0.27061123\n",
      "Iteration 125, loss = 0.26888643\n",
      "Iteration 126, loss = 0.26706287\n",
      "Iteration 127, loss = 0.26366743\n",
      "Iteration 128, loss = 0.26268533\n",
      "Iteration 129, loss = 0.26144867\n",
      "Iteration 130, loss = 0.25878573\n",
      "Iteration 131, loss = 0.25644242\n",
      "Iteration 132, loss = 0.25386275\n",
      "Iteration 133, loss = 0.25148888\n",
      "Iteration 134, loss = 0.24973267\n",
      "Iteration 135, loss = 0.24765125\n",
      "Iteration 136, loss = 0.24611181\n",
      "Iteration 137, loss = 0.24322532\n",
      "Iteration 138, loss = 0.24198225\n",
      "Iteration 139, loss = 0.23908567\n",
      "Iteration 140, loss = 0.23719081\n",
      "Iteration 141, loss = 0.23545450\n",
      "Iteration 142, loss = 0.23349194\n",
      "Iteration 143, loss = 0.23181125\n",
      "Iteration 144, loss = 0.23022012\n",
      "Iteration 145, loss = 0.22808630\n",
      "Iteration 146, loss = 0.22659818\n",
      "Iteration 147, loss = 0.22450143\n",
      "Iteration 148, loss = 0.22301184\n",
      "Iteration 149, loss = 0.22130051\n",
      "Iteration 150, loss = 0.22058525\n",
      "Iteration 151, loss = 0.21695640\n",
      "Iteration 152, loss = 0.21679833\n",
      "Iteration 153, loss = 0.21419910\n",
      "Iteration 154, loss = 0.21240737\n",
      "Iteration 155, loss = 0.21036835\n",
      "Iteration 156, loss = 0.20863220\n",
      "Iteration 157, loss = 0.20791873\n",
      "Iteration 158, loss = 0.20624681\n",
      "Iteration 159, loss = 0.20498415\n",
      "Iteration 160, loss = 0.20236426\n",
      "Iteration 161, loss = 0.20202567\n",
      "Iteration 162, loss = 0.19947921\n",
      "Iteration 163, loss = 0.19809382\n",
      "Iteration 164, loss = 0.19659851\n",
      "Iteration 165, loss = 0.19474377\n",
      "Iteration 166, loss = 0.19306307\n",
      "Iteration 167, loss = 0.19239124\n",
      "Iteration 168, loss = 0.19018147\n",
      "Iteration 169, loss = 0.18925842\n",
      "Iteration 170, loss = 0.18831620\n",
      "Iteration 171, loss = 0.18617462\n",
      "Iteration 172, loss = 0.18549954\n",
      "Iteration 173, loss = 0.18361739\n",
      "Iteration 174, loss = 0.18197183\n",
      "Iteration 175, loss = 0.18106411\n",
      "Iteration 176, loss = 0.18003753\n",
      "Iteration 177, loss = 0.17764875\n",
      "Iteration 178, loss = 0.17653663\n",
      "Iteration 179, loss = 0.17612974\n",
      "Iteration 180, loss = 0.17413393\n",
      "Iteration 181, loss = 0.17456773\n",
      "Iteration 182, loss = 0.17287475\n",
      "Iteration 183, loss = 0.17130285\n",
      "Iteration 184, loss = 0.16993906\n",
      "Iteration 185, loss = 0.16878758\n",
      "Iteration 186, loss = 0.16662819\n",
      "Iteration 187, loss = 0.16644201\n",
      "Iteration 188, loss = 0.16526638\n",
      "Iteration 189, loss = 0.16382271\n",
      "Iteration 190, loss = 0.16487478\n",
      "Iteration 191, loss = 0.16135089\n",
      "Iteration 192, loss = 0.15970483\n",
      "Iteration 193, loss = 0.15992207\n",
      "Iteration 194, loss = 0.15937200\n",
      "Iteration 195, loss = 0.15712249\n",
      "Iteration 196, loss = 0.15605741\n",
      "Iteration 197, loss = 0.15595115\n",
      "Iteration 198, loss = 0.15356089\n",
      "Iteration 199, loss = 0.15305714\n",
      "Iteration 200, loss = 0.15233970\n",
      "Iteration 201, loss = 0.15041651\n",
      "Iteration 202, loss = 0.14987187\n",
      "Iteration 203, loss = 0.14896539\n",
      "Iteration 204, loss = 0.14822609\n",
      "Iteration 205, loss = 0.14690516\n",
      "Iteration 206, loss = 0.14877425\n",
      "Iteration 207, loss = 0.14516826\n",
      "Iteration 208, loss = 0.14313679\n",
      "Iteration 209, loss = 0.14467380\n",
      "Iteration 210, loss = 0.14195014\n",
      "Iteration 211, loss = 0.14242506\n",
      "Iteration 212, loss = 0.14004999\n",
      "Iteration 213, loss = 0.13955296\n",
      "Iteration 214, loss = 0.13833524\n",
      "Iteration 215, loss = 0.13810266\n",
      "Iteration 216, loss = 0.13719121\n",
      "Iteration 217, loss = 0.13623310\n",
      "Iteration 218, loss = 0.13484128\n",
      "Iteration 219, loss = 0.13538216\n",
      "Iteration 220, loss = 0.13331344\n",
      "Iteration 221, loss = 0.13218500\n",
      "Iteration 222, loss = 0.13221559\n",
      "Iteration 223, loss = 0.13064510\n",
      "Iteration 224, loss = 0.12979207\n",
      "Iteration 225, loss = 0.12915103\n",
      "Iteration 226, loss = 0.12897453\n",
      "Iteration 227, loss = 0.12839225\n",
      "Iteration 228, loss = 0.12693439\n",
      "Iteration 229, loss = 0.12601039\n",
      "Iteration 230, loss = 0.12569780\n",
      "Iteration 231, loss = 0.12425706\n",
      "Iteration 232, loss = 0.12448758\n",
      "Iteration 233, loss = 0.12237132\n",
      "Iteration 234, loss = 0.12164318\n",
      "Iteration 235, loss = 0.12114700\n",
      "Iteration 236, loss = 0.12113100\n",
      "Iteration 237, loss = 0.11975579\n",
      "Iteration 238, loss = 0.12035573\n",
      "Iteration 239, loss = 0.11900451\n",
      "Iteration 240, loss = 0.11807436\n",
      "Iteration 241, loss = 0.11839946\n",
      "Iteration 242, loss = 0.11645508\n",
      "Iteration 243, loss = 0.11499946\n",
      "Iteration 244, loss = 0.11738305\n",
      "Iteration 245, loss = 0.11451628\n",
      "Iteration 246, loss = 0.11371480\n",
      "Iteration 247, loss = 0.11278942\n",
      "Iteration 248, loss = 0.11223915\n",
      "Iteration 249, loss = 0.11212158\n",
      "Iteration 250, loss = 0.11328311\n",
      "Iteration 251, loss = 0.11154362\n",
      "Iteration 252, loss = 0.10993422\n",
      "Iteration 253, loss = 0.10939097\n",
      "Iteration 254, loss = 0.10906887\n",
      "Iteration 255, loss = 0.10790285\n",
      "Iteration 256, loss = 0.10742499\n",
      "Iteration 257, loss = 0.10605221\n",
      "Iteration 258, loss = 0.10681528\n",
      "Iteration 259, loss = 0.10547152\n",
      "Iteration 260, loss = 0.10498317\n",
      "Iteration 261, loss = 0.10367487\n",
      "Iteration 262, loss = 0.10460557\n",
      "Iteration 263, loss = 0.10305875\n",
      "Iteration 264, loss = 0.10182151\n",
      "Iteration 265, loss = 0.10113695\n",
      "Iteration 266, loss = 0.10193581\n",
      "Iteration 267, loss = 0.10051035\n",
      "Iteration 268, loss = 0.09986502\n",
      "Iteration 269, loss = 0.09925432\n",
      "Iteration 270, loss = 0.09867578\n",
      "Iteration 271, loss = 0.09890666\n",
      "Iteration 272, loss = 0.09885564\n",
      "Iteration 273, loss = 0.09703935\n",
      "Iteration 274, loss = 0.09646526\n",
      "Iteration 275, loss = 0.09579007\n",
      "Iteration 276, loss = 0.09545098\n",
      "Iteration 277, loss = 0.09452137\n",
      "Iteration 278, loss = 0.09514449\n",
      "Iteration 279, loss = 0.09391495\n",
      "Iteration 280, loss = 0.09474280\n",
      "Iteration 281, loss = 0.09368178\n",
      "Iteration 282, loss = 0.09225985\n",
      "Iteration 283, loss = 0.09177975\n",
      "Iteration 284, loss = 0.09107964\n",
      "Iteration 285, loss = 0.09060966\n",
      "Iteration 286, loss = 0.09131896\n",
      "Iteration 287, loss = 0.08950804\n",
      "Iteration 288, loss = 0.08929355\n",
      "Iteration 289, loss = 0.08874432\n",
      "Iteration 290, loss = 0.08784834\n",
      "Iteration 291, loss = 0.08780496\n",
      "Iteration 292, loss = 0.08779668\n",
      "Iteration 293, loss = 0.08792315\n",
      "Iteration 294, loss = 0.08642154\n",
      "Iteration 295, loss = 0.08705565\n",
      "Iteration 296, loss = 0.08588303\n",
      "Iteration 297, loss = 0.08504762\n",
      "Iteration 298, loss = 0.08452543\n",
      "Iteration 299, loss = 0.08382808\n",
      "Iteration 300, loss = 0.08412393\n",
      "Iteration 301, loss = 0.08367294\n",
      "Iteration 302, loss = 0.08301571\n",
      "Iteration 303, loss = 0.08190697\n",
      "Iteration 304, loss = 0.08207876\n",
      "Iteration 305, loss = 0.08166413\n",
      "Iteration 306, loss = 0.08119671\n",
      "Iteration 307, loss = 0.08084954\n",
      "Iteration 308, loss = 0.08097242\n",
      "Iteration 309, loss = 0.07980482\n",
      "Iteration 310, loss = 0.07932050\n",
      "Iteration 311, loss = 0.07846401\n",
      "Iteration 312, loss = 0.07901398\n",
      "Iteration 313, loss = 0.07839063\n",
      "Iteration 314, loss = 0.07821515\n",
      "Iteration 315, loss = 0.07676435\n",
      "Iteration 316, loss = 0.07699541\n",
      "Iteration 317, loss = 0.07638488\n",
      "Iteration 318, loss = 0.07622416\n",
      "Iteration 319, loss = 0.07518541\n",
      "Iteration 320, loss = 0.07512960\n",
      "Iteration 321, loss = 0.07572894\n",
      "Iteration 322, loss = 0.07385930\n",
      "Iteration 323, loss = 0.07401912\n",
      "Iteration 324, loss = 0.07560819\n",
      "Iteration 325, loss = 0.07382138\n",
      "Iteration 326, loss = 0.07422337\n",
      "Iteration 327, loss = 0.07285935\n",
      "Iteration 328, loss = 0.07215313\n",
      "Iteration 329, loss = 0.07103201\n",
      "Iteration 330, loss = 0.07096406\n",
      "Iteration 331, loss = 0.07086781\n",
      "Iteration 332, loss = 0.07018065\n",
      "Iteration 333, loss = 0.07020376\n",
      "Iteration 334, loss = 0.06982941\n",
      "Iteration 335, loss = 0.06919305\n",
      "Iteration 336, loss = 0.06909032\n",
      "Iteration 337, loss = 0.06875524\n",
      "Iteration 338, loss = 0.06982512\n",
      "Iteration 339, loss = 0.06856700\n",
      "Iteration 340, loss = 0.06888320\n",
      "Iteration 341, loss = 0.06732386\n",
      "Iteration 342, loss = 0.06699017\n",
      "Iteration 343, loss = 0.06686686\n",
      "Iteration 344, loss = 0.06580448\n",
      "Iteration 345, loss = 0.06674505\n",
      "Iteration 346, loss = 0.06564907\n",
      "Iteration 347, loss = 0.06684206\n",
      "Iteration 348, loss = 0.06551047\n",
      "Iteration 349, loss = 0.06518448\n",
      "Iteration 350, loss = 0.06464164\n",
      "Iteration 351, loss = 0.06416135\n",
      "Iteration 352, loss = 0.06397811\n",
      "Iteration 353, loss = 0.06322618\n",
      "Iteration 354, loss = 0.06300417\n",
      "Iteration 355, loss = 0.06259446\n",
      "Iteration 356, loss = 0.06200641\n",
      "Iteration 357, loss = 0.06223836\n",
      "Iteration 358, loss = 0.06257538\n",
      "Iteration 359, loss = 0.06113394\n",
      "Iteration 360, loss = 0.06189608\n",
      "Iteration 361, loss = 0.06253768\n",
      "Iteration 362, loss = 0.06096101\n",
      "Iteration 363, loss = 0.06022072\n",
      "Iteration 364, loss = 0.05981386\n",
      "Iteration 365, loss = 0.05906884\n",
      "Iteration 366, loss = 0.05949769\n",
      "Iteration 367, loss = 0.05918892\n",
      "Iteration 368, loss = 0.05818288\n",
      "Iteration 369, loss = 0.05784264\n",
      "Iteration 370, loss = 0.05793531\n",
      "Iteration 371, loss = 0.05750930\n",
      "Iteration 372, loss = 0.05711554\n",
      "Iteration 373, loss = 0.05698197\n",
      "Iteration 374, loss = 0.05765920\n",
      "Iteration 375, loss = 0.05688800\n",
      "Iteration 376, loss = 0.05719317\n",
      "Iteration 377, loss = 0.05618416\n",
      "Iteration 378, loss = 0.05572226\n",
      "Iteration 379, loss = 0.05553813\n",
      "Iteration 380, loss = 0.05540609\n",
      "Iteration 381, loss = 0.05542573\n",
      "Iteration 382, loss = 0.05426588\n",
      "Iteration 383, loss = 0.05410160\n",
      "Iteration 384, loss = 0.05453390\n",
      "Iteration 385, loss = 0.05362976\n",
      "Iteration 386, loss = 0.05354042\n",
      "Iteration 387, loss = 0.05291415\n",
      "Iteration 388, loss = 0.05279163\n",
      "Iteration 389, loss = 0.05255638\n",
      "Iteration 390, loss = 0.05204635\n",
      "Iteration 391, loss = 0.05259254\n",
      "Iteration 392, loss = 0.05183058\n",
      "Iteration 393, loss = 0.05146939\n",
      "Iteration 394, loss = 0.05199737\n",
      "Iteration 395, loss = 0.05054436\n",
      "Iteration 396, loss = 0.05060173\n",
      "Iteration 397, loss = 0.05058763\n",
      "Iteration 398, loss = 0.05022539\n",
      "Iteration 399, loss = 0.04993408\n",
      "Iteration 400, loss = 0.05012330\n",
      "Iteration 401, loss = 0.04994534\n",
      "Iteration 402, loss = 0.04950490\n",
      "Iteration 403, loss = 0.04913575\n",
      "Iteration 404, loss = 0.04909620\n",
      "Iteration 405, loss = 0.04824056\n",
      "Iteration 406, loss = 0.04854353\n",
      "Iteration 407, loss = 0.04957304\n",
      "Iteration 408, loss = 0.04798556\n",
      "Iteration 409, loss = 0.04738495\n",
      "Iteration 410, loss = 0.04823136\n",
      "Iteration 411, loss = 0.04809825\n",
      "Iteration 412, loss = 0.04707264\n",
      "Iteration 413, loss = 0.04617190\n",
      "Iteration 414, loss = 0.04629782\n",
      "Iteration 415, loss = 0.04623256\n",
      "Iteration 416, loss = 0.04551689\n",
      "Iteration 417, loss = 0.04629304\n",
      "Iteration 418, loss = 0.04544669\n",
      "Iteration 419, loss = 0.04501376\n",
      "Iteration 420, loss = 0.04496872\n",
      "Iteration 421, loss = 0.04459827\n",
      "Iteration 422, loss = 0.04534468\n",
      "Iteration 423, loss = 0.04454210\n",
      "Iteration 424, loss = 0.04402962\n",
      "Iteration 425, loss = 0.04425346\n",
      "Iteration 426, loss = 0.04386123\n",
      "Iteration 427, loss = 0.04350380\n",
      "Iteration 428, loss = 0.04361536\n",
      "Iteration 429, loss = 0.04277747\n",
      "Iteration 430, loss = 0.04266613\n",
      "Iteration 431, loss = 0.04265337\n",
      "Iteration 432, loss = 0.04249080\n",
      "Iteration 433, loss = 0.04286323\n",
      "Iteration 434, loss = 0.04255806\n",
      "Iteration 435, loss = 0.04227992\n",
      "Iteration 436, loss = 0.04221609\n",
      "Iteration 437, loss = 0.04146344\n",
      "Iteration 438, loss = 0.04152998\n",
      "Iteration 439, loss = 0.04117794\n",
      "Iteration 440, loss = 0.04085719\n",
      "Iteration 441, loss = 0.04087626\n",
      "Iteration 442, loss = 0.04118201\n",
      "Iteration 443, loss = 0.04038840\n",
      "Iteration 444, loss = 0.04000923\n",
      "Iteration 445, loss = 0.04008209\n",
      "Iteration 446, loss = 0.03970153\n",
      "Iteration 447, loss = 0.03939233\n",
      "Iteration 448, loss = 0.03984437\n",
      "Iteration 449, loss = 0.03911823\n",
      "Iteration 450, loss = 0.03936225\n",
      "Iteration 451, loss = 0.03864991\n",
      "Iteration 452, loss = 0.03917939\n",
      "Iteration 453, loss = 0.03885133\n",
      "Iteration 454, loss = 0.03853918\n",
      "Iteration 455, loss = 0.03906862\n",
      "Iteration 456, loss = 0.03856378\n",
      "Iteration 457, loss = 0.03804279\n",
      "Iteration 458, loss = 0.03783596\n",
      "Iteration 459, loss = 0.03744122\n",
      "Iteration 460, loss = 0.03732820\n",
      "Iteration 461, loss = 0.03690908\n",
      "Iteration 462, loss = 0.03691480\n",
      "Iteration 463, loss = 0.03648117\n",
      "Iteration 464, loss = 0.03657216\n",
      "Iteration 465, loss = 0.03658679\n",
      "Iteration 466, loss = 0.03619454\n",
      "Iteration 467, loss = 0.03600390\n",
      "Iteration 468, loss = 0.03614531\n",
      "Iteration 469, loss = 0.03557512\n",
      "Iteration 470, loss = 0.03550722\n",
      "Iteration 471, loss = 0.03541440\n",
      "Iteration 472, loss = 0.03551948\n",
      "Iteration 473, loss = 0.03500146\n",
      "Iteration 474, loss = 0.03507930\n",
      "Iteration 475, loss = 0.03488263\n",
      "Iteration 476, loss = 0.03486468\n",
      "Iteration 477, loss = 0.03468446\n",
      "Iteration 478, loss = 0.03417522\n",
      "Iteration 479, loss = 0.03413766\n",
      "Iteration 480, loss = 0.03442173\n",
      "Iteration 481, loss = 0.03369113\n",
      "Iteration 482, loss = 0.03400347\n",
      "Iteration 483, loss = 0.03378124\n",
      "Iteration 484, loss = 0.03362553\n",
      "Iteration 485, loss = 0.03328045\n",
      "Iteration 486, loss = 0.03340609\n",
      "Iteration 487, loss = 0.03296445\n",
      "Iteration 488, loss = 0.03331300\n",
      "Iteration 489, loss = 0.03307604\n",
      "Iteration 490, loss = 0.03252757\n",
      "Iteration 491, loss = 0.03230752\n",
      "Iteration 492, loss = 0.03233672\n",
      "Iteration 493, loss = 0.03259391\n",
      "Iteration 494, loss = 0.03199069\n",
      "Iteration 495, loss = 0.03202702\n",
      "Iteration 496, loss = 0.03201244\n",
      "Iteration 497, loss = 0.03195806\n",
      "Iteration 498, loss = 0.03127706\n",
      "Iteration 499, loss = 0.03110913\n",
      "Iteration 500, loss = 0.03129774\n",
      "Iteration 501, loss = 0.03108963\n",
      "Iteration 502, loss = 0.03095097\n",
      "Iteration 503, loss = 0.03122132\n",
      "Iteration 504, loss = 0.03140028\n",
      "Iteration 505, loss = 0.03044590\n",
      "Iteration 506, loss = 0.03028515\n",
      "Iteration 507, loss = 0.03020239\n",
      "Iteration 508, loss = 0.03021399\n",
      "Iteration 509, loss = 0.03035871\n",
      "Iteration 510, loss = 0.02997426\n",
      "Iteration 511, loss = 0.02973856\n",
      "Iteration 512, loss = 0.02957231\n",
      "Iteration 513, loss = 0.02952806\n",
      "Iteration 514, loss = 0.02946963\n",
      "Iteration 515, loss = 0.02984906\n",
      "Iteration 516, loss = 0.02931529\n",
      "Iteration 517, loss = 0.02905154\n",
      "Iteration 518, loss = 0.02884582\n",
      "Iteration 519, loss = 0.02888990\n",
      "Iteration 520, loss = 0.02877355\n",
      "Iteration 521, loss = 0.02882948\n",
      "Iteration 522, loss = 0.02909965\n",
      "Iteration 523, loss = 0.02859835\n",
      "Iteration 524, loss = 0.02861896\n",
      "Iteration 525, loss = 0.02829278\n",
      "Iteration 526, loss = 0.02845043\n",
      "Iteration 527, loss = 0.02785056\n",
      "Iteration 528, loss = 0.02767522\n",
      "Iteration 529, loss = 0.02772244\n",
      "Iteration 530, loss = 0.02775798\n",
      "Iteration 531, loss = 0.02789571\n",
      "Iteration 532, loss = 0.02735487\n",
      "Iteration 533, loss = 0.02717546\n",
      "Iteration 534, loss = 0.02716325\n",
      "Iteration 535, loss = 0.02731525\n",
      "Iteration 536, loss = 0.02687006\n",
      "Iteration 537, loss = 0.02730157\n",
      "Iteration 538, loss = 0.02670578\n",
      "Iteration 539, loss = 0.02670338\n",
      "Iteration 540, loss = 0.02687792\n",
      "Iteration 541, loss = 0.02653874\n",
      "Iteration 542, loss = 0.02618491\n",
      "Iteration 543, loss = 0.02616643\n",
      "Iteration 544, loss = 0.02623882\n",
      "Iteration 545, loss = 0.02607417\n",
      "Iteration 546, loss = 0.02723320\n",
      "Iteration 547, loss = 0.02624820\n",
      "Iteration 548, loss = 0.02653521\n",
      "Iteration 549, loss = 0.02563354\n",
      "Iteration 550, loss = 0.02573282\n",
      "Iteration 551, loss = 0.02569528\n",
      "Iteration 552, loss = 0.02530490\n",
      "Iteration 553, loss = 0.02521469\n",
      "Iteration 554, loss = 0.02522518\n",
      "Iteration 555, loss = 0.02545008\n",
      "Iteration 556, loss = 0.02526941\n",
      "Iteration 557, loss = 0.02490025\n",
      "Iteration 558, loss = 0.02480836\n",
      "Iteration 559, loss = 0.02480621\n",
      "Iteration 560, loss = 0.02453583\n",
      "Iteration 561, loss = 0.02448390\n",
      "Iteration 562, loss = 0.02432466\n",
      "Iteration 563, loss = 0.02420260\n",
      "Iteration 564, loss = 0.02430525\n",
      "Iteration 565, loss = 0.02465523\n",
      "Iteration 566, loss = 0.02418028\n",
      "Iteration 567, loss = 0.02397496\n",
      "Iteration 568, loss = 0.02382751\n",
      "Iteration 569, loss = 0.02381213\n",
      "Iteration 570, loss = 0.02385570\n",
      "Iteration 571, loss = 0.02356396\n",
      "Iteration 572, loss = 0.02366102\n",
      "Iteration 573, loss = 0.02344215\n",
      "Iteration 574, loss = 0.02476657\n",
      "Iteration 575, loss = 0.02521940\n",
      "Iteration 576, loss = 0.02336371\n",
      "Iteration 577, loss = 0.02321042\n",
      "Iteration 578, loss = 0.02318700\n",
      "Iteration 579, loss = 0.02291795\n",
      "Iteration 580, loss = 0.02293380\n",
      "Iteration 581, loss = 0.02315941\n",
      "Iteration 582, loss = 0.02252093\n",
      "Iteration 583, loss = 0.02261343\n",
      "Iteration 584, loss = 0.02264628\n",
      "Iteration 585, loss = 0.02236225\n",
      "Iteration 586, loss = 0.02270331\n",
      "Iteration 587, loss = 0.02262277\n",
      "Iteration 588, loss = 0.02233864\n",
      "Iteration 589, loss = 0.02236966\n",
      "Iteration 590, loss = 0.02237897\n",
      "Iteration 591, loss = 0.02227394\n",
      "Iteration 592, loss = 0.02203939\n",
      "Iteration 593, loss = 0.02201271\n",
      "Iteration 594, loss = 0.02201934\n",
      "Iteration 595, loss = 0.02177845\n",
      "Iteration 596, loss = 0.02161861\n",
      "Iteration 597, loss = 0.02174144\n",
      "Iteration 598, loss = 0.02150155\n",
      "Iteration 599, loss = 0.02131987\n",
      "Iteration 600, loss = 0.02174046\n",
      "Iteration 601, loss = 0.02121866\n",
      "Iteration 602, loss = 0.02114951\n",
      "Iteration 603, loss = 0.02141532\n",
      "Iteration 604, loss = 0.02136341\n",
      "Iteration 605, loss = 0.02112668\n",
      "Iteration 606, loss = 0.02098175\n",
      "Iteration 607, loss = 0.02072148\n",
      "Iteration 608, loss = 0.02090644\n",
      "Iteration 609, loss = 0.02080958\n",
      "Iteration 610, loss = 0.02071548\n",
      "Iteration 611, loss = 0.02048076\n",
      "Iteration 612, loss = 0.02065596\n",
      "Iteration 613, loss = 0.02111328\n",
      "Iteration 614, loss = 0.02069112\n",
      "Iteration 615, loss = 0.02065980\n",
      "Iteration 616, loss = 0.02017946\n",
      "Iteration 617, loss = 0.02045456\n",
      "Iteration 618, loss = 0.02022719\n",
      "Iteration 619, loss = 0.01997506\n",
      "Iteration 620, loss = 0.02010803\n",
      "Iteration 621, loss = 0.01981152\n",
      "Iteration 622, loss = 0.01990488\n",
      "Iteration 623, loss = 0.01981475\n",
      "Iteration 624, loss = 0.01981238\n",
      "Iteration 625, loss = 0.01959456\n",
      "Iteration 626, loss = 0.01951597\n",
      "Iteration 627, loss = 0.01959570\n",
      "Iteration 628, loss = 0.01956435\n",
      "Iteration 629, loss = 0.01993554\n",
      "Iteration 630, loss = 0.01943491\n",
      "Iteration 631, loss = 0.01984038\n",
      "Iteration 632, loss = 0.01931525\n",
      "Iteration 633, loss = 0.01921841\n",
      "Iteration 634, loss = 0.01897293\n",
      "Iteration 635, loss = 0.01892456\n",
      "Iteration 636, loss = 0.01885411\n",
      "Iteration 637, loss = 0.01870540\n",
      "Iteration 638, loss = 0.01908979\n",
      "Iteration 639, loss = 0.01874708\n",
      "Iteration 640, loss = 0.01891892\n",
      "Iteration 641, loss = 0.01875772\n",
      "Iteration 642, loss = 0.01865437\n",
      "Iteration 643, loss = 0.01836591\n",
      "Iteration 644, loss = 0.01857376\n",
      "Iteration 645, loss = 0.01858294\n",
      "Iteration 646, loss = 0.01839713\n",
      "Iteration 647, loss = 0.01854139\n",
      "Iteration 648, loss = 0.01831193\n",
      "Iteration 649, loss = 0.01816406\n",
      "Iteration 650, loss = 0.01811832\n",
      "Iteration 651, loss = 0.01813559\n",
      "Iteration 652, loss = 0.01792179\n",
      "Iteration 653, loss = 0.01841865\n",
      "Iteration 654, loss = 0.01790500\n",
      "Iteration 655, loss = 0.01785045\n",
      "Iteration 656, loss = 0.01779496\n",
      "Iteration 657, loss = 0.01780549\n",
      "Iteration 658, loss = 0.01788352\n",
      "Iteration 659, loss = 0.01755146\n",
      "Iteration 660, loss = 0.01768501\n",
      "Iteration 661, loss = 0.01804632\n",
      "Iteration 662, loss = 0.01765393\n",
      "Iteration 663, loss = 0.01729019\n",
      "Iteration 664, loss = 0.01739236\n",
      "Iteration 665, loss = 0.01736740\n",
      "Iteration 666, loss = 0.01718287\n",
      "Iteration 667, loss = 0.01759929\n",
      "Iteration 668, loss = 0.01719436\n",
      "Iteration 669, loss = 0.01767050\n",
      "Iteration 670, loss = 0.01724323\n",
      "Iteration 671, loss = 0.01726157\n",
      "Iteration 672, loss = 0.01713032\n",
      "Iteration 673, loss = 0.01696985\n",
      "Iteration 674, loss = 0.01693671\n",
      "Iteration 675, loss = 0.01673090\n",
      "Iteration 676, loss = 0.01675633\n",
      "Iteration 677, loss = 0.01694790\n",
      "Iteration 678, loss = 0.01659428\n",
      "Iteration 679, loss = 0.01685210\n",
      "Iteration 680, loss = 0.01654580\n",
      "Iteration 681, loss = 0.01635829\n",
      "Iteration 682, loss = 0.01652670\n",
      "Iteration 683, loss = 0.01654375\n",
      "Iteration 684, loss = 0.01628742\n",
      "Iteration 685, loss = 0.01634245\n",
      "Iteration 686, loss = 0.01617236\n",
      "Iteration 687, loss = 0.01617278\n",
      "Iteration 688, loss = 0.01640114\n",
      "Iteration 689, loss = 0.01623597\n",
      "Iteration 690, loss = 0.01615530\n",
      "Iteration 691, loss = 0.01599362\n",
      "Iteration 692, loss = 0.01617592\n",
      "Iteration 693, loss = 0.01600266\n",
      "Iteration 694, loss = 0.01595328\n",
      "Iteration 695, loss = 0.01610423\n",
      "Iteration 696, loss = 0.01613931\n",
      "Iteration 697, loss = 0.01580733\n",
      "Iteration 698, loss = 0.01586557\n",
      "Iteration 699, loss = 0.01631994\n",
      "Iteration 700, loss = 0.01582284\n",
      "Iteration 701, loss = 0.01572734\n",
      "Iteration 702, loss = 0.01551000\n",
      "Iteration 703, loss = 0.01566577\n",
      "Iteration 704, loss = 0.01557979\n",
      "Iteration 705, loss = 0.01566911\n",
      "Iteration 706, loss = 0.01547924\n",
      "Iteration 707, loss = 0.01542095\n",
      "Iteration 708, loss = 0.01525811\n",
      "Iteration 709, loss = 0.01543680\n",
      "Iteration 710, loss = 0.01553239\n",
      "Iteration 711, loss = 0.01518141\n",
      "Iteration 712, loss = 0.01521051\n",
      "Iteration 713, loss = 0.01518802\n",
      "Iteration 714, loss = 0.01502789\n",
      "Iteration 715, loss = 0.01505397\n",
      "Iteration 716, loss = 0.01524520\n",
      "Iteration 717, loss = 0.01509732\n",
      "Iteration 718, loss = 0.01487126\n",
      "Iteration 719, loss = 0.01481418\n",
      "Iteration 720, loss = 0.01497211\n",
      "Iteration 721, loss = 0.01466766\n",
      "Iteration 722, loss = 0.01473915\n",
      "Iteration 723, loss = 0.01490263\n",
      "Iteration 724, loss = 0.01471712\n",
      "Iteration 725, loss = 0.01466330\n",
      "Iteration 726, loss = 0.01481600\n",
      "Iteration 727, loss = 0.01473112\n",
      "Iteration 728, loss = 0.01468573\n",
      "Iteration 729, loss = 0.01435989\n",
      "Iteration 730, loss = 0.01432876\n",
      "Iteration 731, loss = 0.01446413\n",
      "Iteration 732, loss = 0.01441991\n",
      "Iteration 733, loss = 0.01420831\n",
      "Iteration 734, loss = 0.01412025\n",
      "Iteration 735, loss = 0.01416005\n",
      "Iteration 736, loss = 0.01421996\n",
      "Iteration 737, loss = 0.01434152\n",
      "Iteration 738, loss = 0.01410493\n",
      "Iteration 739, loss = 0.01400746\n",
      "Iteration 740, loss = 0.01394307\n",
      "Iteration 741, loss = 0.01387919\n",
      "Iteration 742, loss = 0.01389082\n",
      "Iteration 743, loss = 0.01389437\n",
      "Iteration 744, loss = 0.01468939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_inputs = get_parameters(train_inputs)\n",
    "\n",
    "train_inputs = np.array(train_inputs)\n",
    "train_outputs = np.array(train_outputs)\n",
    "\n",
    "test_inputs = np.array(test_inputs)\n",
    "test_outputs = np.array(test_outputs)\n",
    "\n",
    "train_inputs = train_inputs / 255.0\n",
    "classifier = neural_network.MLPClassifier(hidden_layer_sizes=(32,32), activation='relu', max_iter=1000, solver='sgd', verbose=1)\n",
    "classifier.fit(train_inputs, train_outputs)\n",
    "\n",
    "predicted = get_parameters(test_inputs)\n",
    "predicted = np.array(predicted)\n",
    "predicted = predicted / 255.0\n",
    "outputs = classifier.predict(predicted)\n",
    "\n",
    "plt.plot(classifier.loss_curve_, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8772455089820359\n",
      "Precision: 0.8776150888611113\n",
      "Recall: 0.8979591836734694\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(test_outputs, outputs)\n",
    "prec = precision_score(test_outputs, outputs, average=\"weighted\")\n",
    "recall = recall_score(test_outputs, outputs)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
